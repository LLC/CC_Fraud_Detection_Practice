{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TXKEY</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T43428</td>\n",
       "      <td>41505.0</td>\n",
       "      <td>-16.526507</td>\n",
       "      <td>8.584972</td>\n",
       "      <td>-18.649853</td>\n",
       "      <td>9.505594</td>\n",
       "      <td>-13.793819</td>\n",
       "      <td>-2.832404</td>\n",
       "      <td>-16.701694</td>\n",
       "      <td>7.517344</td>\n",
       "      <td>...</td>\n",
       "      <td>1.190739</td>\n",
       "      <td>-1.127670</td>\n",
       "      <td>-2.358579</td>\n",
       "      <td>0.673461</td>\n",
       "      <td>-1.413700</td>\n",
       "      <td>-0.462762</td>\n",
       "      <td>-2.018575</td>\n",
       "      <td>-1.042804</td>\n",
       "      <td>364.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T49906</td>\n",
       "      <td>44261.0</td>\n",
       "      <td>0.339812</td>\n",
       "      <td>-2.743745</td>\n",
       "      <td>-0.134070</td>\n",
       "      <td>-1.385729</td>\n",
       "      <td>-1.451413</td>\n",
       "      <td>1.015887</td>\n",
       "      <td>-0.524379</td>\n",
       "      <td>0.224060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213436</td>\n",
       "      <td>-0.942525</td>\n",
       "      <td>-0.526819</td>\n",
       "      <td>-1.156992</td>\n",
       "      <td>0.311211</td>\n",
       "      <td>-0.746647</td>\n",
       "      <td>0.040996</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>520.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T29474</td>\n",
       "      <td>35484.0</td>\n",
       "      <td>1.399590</td>\n",
       "      <td>-0.590701</td>\n",
       "      <td>0.168619</td>\n",
       "      <td>-1.029950</td>\n",
       "      <td>-0.539806</td>\n",
       "      <td>0.040444</td>\n",
       "      <td>-0.712567</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102398</td>\n",
       "      <td>0.168269</td>\n",
       "      <td>-0.166639</td>\n",
       "      <td>-0.810250</td>\n",
       "      <td>0.505083</td>\n",
       "      <td>-0.232340</td>\n",
       "      <td>0.011409</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>31.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T276481</td>\n",
       "      <td>167123.0</td>\n",
       "      <td>-0.432071</td>\n",
       "      <td>1.647895</td>\n",
       "      <td>-1.669361</td>\n",
       "      <td>-0.349504</td>\n",
       "      <td>0.785785</td>\n",
       "      <td>-0.630647</td>\n",
       "      <td>0.276990</td>\n",
       "      <td>0.586025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358932</td>\n",
       "      <td>0.873663</td>\n",
       "      <td>-0.178642</td>\n",
       "      <td>-0.017171</td>\n",
       "      <td>-0.207392</td>\n",
       "      <td>-0.157756</td>\n",
       "      <td>-0.237386</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T278846</td>\n",
       "      <td>168473.0</td>\n",
       "      <td>2.014160</td>\n",
       "      <td>-0.137394</td>\n",
       "      <td>-1.015839</td>\n",
       "      <td>0.327269</td>\n",
       "      <td>-0.182179</td>\n",
       "      <td>-0.956571</td>\n",
       "      <td>0.043241</td>\n",
       "      <td>-0.160746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.238644</td>\n",
       "      <td>-0.616400</td>\n",
       "      <td>0.347045</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>-0.360196</td>\n",
       "      <td>0.174730</td>\n",
       "      <td>-0.078043</td>\n",
       "      <td>-0.070571</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TXKEY      Time         V1        V2         V3        V4         V5  \\\n",
       "0   T43428   41505.0 -16.526507  8.584972 -18.649853  9.505594 -13.793819   \n",
       "1   T49906   44261.0   0.339812 -2.743745  -0.134070 -1.385729  -1.451413   \n",
       "2   T29474   35484.0   1.399590 -0.590701   0.168619 -1.029950  -0.539806   \n",
       "3  T276481  167123.0  -0.432071  1.647895  -1.669361 -0.349504   0.785785   \n",
       "4  T278846  168473.0   2.014160 -0.137394  -1.015839  0.327269  -0.182179   \n",
       "\n",
       "         V6         V7        V8  ...         V21       V22       V23  \\\n",
       "0 -2.832404 -16.701694  7.517344  ...    1.190739 -1.127670 -2.358579   \n",
       "1  1.015887  -0.524379  0.224060  ...   -0.213436 -0.942525 -0.526819   \n",
       "2  0.040444  -0.712567  0.002299  ...    0.102398  0.168269 -0.166639   \n",
       "3 -0.630647   0.276990  0.586025  ...    0.358932  0.873663 -0.178642   \n",
       "4 -0.956571   0.043241 -0.160746  ...   -0.238644 -0.616400  0.347045   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  Class  \n",
       "0  0.673461 -1.413700 -0.462762 -2.018575 -1.042804  364.19      1  \n",
       "1 -1.156992  0.311211 -0.746647  0.040996  0.102038  520.12      0  \n",
       "2 -0.810250  0.505083 -0.232340  0.011409  0.004634   31.00      0  \n",
       "3 -0.017171 -0.207392 -0.157756 -0.237386  0.001934    1.50      0  \n",
       "4  0.061561 -0.360196  0.174730 -0.078043 -0.070571    0.89      0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df_train.drop(['TXKEY','Class'],axis=1)\n",
    "y = df_train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T2', 'T5', 'T7', ..., 'T284797', 'T284800', 'T284802'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TXKEY =np.array(df_test[\"TXKEY\"])\n",
    "TXKEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = df_test.drop(['TXKEY'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LightGBM_GBDT_plus_LR(x,y):\n",
    "    \n",
    "    predictions = 0\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        # create dataset for lightgbm\n",
    "        lgb_train = lgb.Dataset(x,y)\n",
    "\n",
    "        params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': {'binary_logloss'},\n",
    "        'num_leaves': 64,\n",
    "        'num_trees': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "        }\n",
    "\n",
    "        # number of leaves,will be used in feature transformation\n",
    "        num_leaf = 64\n",
    "\n",
    "        print('Start training...')\n",
    "        # train\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=300,\n",
    "                        valid_sets=lgb_train)\n",
    "\n",
    "        print('Save model...')\n",
    "        # save model to file\n",
    "        gbm.save_model('model.txt')\n",
    "\n",
    "        print('Start predicting...')\n",
    "        # predict and get data on leaves, training data\n",
    "        y_pred = gbm.predict(x, pred_leaf=True)\n",
    "\n",
    "        print('Writing transformed training data')\n",
    "        transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf],dtype=np.int64)  # N * num_tress * num_leafs\n",
    "        for i in range(0, len(y_pred)):\n",
    "            temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
    "            transformed_training_matrix[i][temp] += 1\n",
    "\n",
    "        lr = LogisticRegression(penalty='l2',C=0.05) # logistic model construction\n",
    "        lr.fit(transformed_training_matrix,y)  # fitting the data\n",
    "        \n",
    "        y_pred = gbm.predict(X_test, pred_leaf=True)\n",
    "        \n",
    "        transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf], dtype=np.int64)\n",
    "        for i in range(0, len(y_pred)):\n",
    "            temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
    "            transformed_testing_matrix[i][temp] += 1\n",
    "        \n",
    "        y_pred_test = lr.predict(transformed_testing_matrix)   # Give the probabilty on each label\n",
    "        \n",
    "        if i == 0:\n",
    "            predictions = y_pred_test\n",
    "        else:\n",
    "            predictions += y_pred_test\n",
    "            \n",
    "    for j in range(len(predictions)):\n",
    "        if predictions[j] >= 5:\n",
    "            predictions[j] = 1\n",
    "        else:\n",
    "            predictions[j] = 0\n",
    "    \n",
    "    my_solution = pd.DataFrame(predictions, TXKEY, columns = [\"Class\"])\n",
    "    my_solution\n",
    "    \n",
    "    my_solution.to_csv(\"Ans/solution_17.csv\", index_label = [\"TXKEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/llc/anaconda2/envs/python3/lib/python3.6/site-packages/lightgbm/engine.py:102: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_logloss: 0.00608694\n",
      "[2]\ttraining's binary_logloss: 0.00578185\n",
      "[3]\ttraining's binary_logloss: 0.00555798\n",
      "[4]\ttraining's binary_logloss: 0.00540876\n",
      "[5]\ttraining's binary_logloss: 0.00525913\n",
      "[6]\ttraining's binary_logloss: 0.00512439\n",
      "[7]\ttraining's binary_logloss: 0.00500932\n",
      "[8]\ttraining's binary_logloss: 0.00489742\n",
      "[9]\ttraining's binary_logloss: 0.00479285\n",
      "[10]\ttraining's binary_logloss: 0.00469912\n",
      "[11]\ttraining's binary_logloss: 0.00459227\n",
      "[12]\ttraining's binary_logloss: 0.00449933\n",
      "[13]\ttraining's binary_logloss: 0.00441153\n",
      "[14]\ttraining's binary_logloss: 0.00432356\n",
      "[15]\ttraining's binary_logloss: 0.00424865\n",
      "[16]\ttraining's binary_logloss: 0.00415845\n",
      "[17]\ttraining's binary_logloss: 0.00407362\n",
      "[18]\ttraining's binary_logloss: 0.00400211\n",
      "[19]\ttraining's binary_logloss: 0.00393314\n",
      "[20]\ttraining's binary_logloss: 0.00386861\n",
      "[21]\ttraining's binary_logloss: 0.00380176\n",
      "[22]\ttraining's binary_logloss: 0.0037397\n",
      "[23]\ttraining's binary_logloss: 0.00368197\n",
      "[24]\ttraining's binary_logloss: 0.00362891\n",
      "[25]\ttraining's binary_logloss: 0.0035756\n",
      "[26]\ttraining's binary_logloss: 0.00351868\n",
      "[27]\ttraining's binary_logloss: 0.00346563\n",
      "[28]\ttraining's binary_logloss: 0.00341614\n",
      "[29]\ttraining's binary_logloss: 0.00336747\n",
      "[30]\ttraining's binary_logloss: 0.00332008\n",
      "[31]\ttraining's binary_logloss: 0.00327494\n",
      "[32]\ttraining's binary_logloss: 0.00322669\n",
      "[33]\ttraining's binary_logloss: 0.00318401\n",
      "[34]\ttraining's binary_logloss: 0.00314252\n",
      "[35]\ttraining's binary_logloss: 0.00309811\n",
      "[36]\ttraining's binary_logloss: 0.0030546\n",
      "[37]\ttraining's binary_logloss: 0.00301313\n",
      "[38]\ttraining's binary_logloss: 0.00297128\n",
      "[39]\ttraining's binary_logloss: 0.00293205\n",
      "[40]\ttraining's binary_logloss: 0.00289466\n",
      "[41]\ttraining's binary_logloss: 0.00285809\n",
      "[42]\ttraining's binary_logloss: 0.00282402\n",
      "[43]\ttraining's binary_logloss: 0.00278886\n",
      "[44]\ttraining's binary_logloss: 0.0027554\n",
      "[45]\ttraining's binary_logloss: 0.00272396\n",
      "[46]\ttraining's binary_logloss: 0.00268889\n",
      "[47]\ttraining's binary_logloss: 0.00265594\n",
      "[48]\ttraining's binary_logloss: 0.00262267\n",
      "[49]\ttraining's binary_logloss: 0.00258951\n",
      "[50]\ttraining's binary_logloss: 0.0025591\n",
      "[51]\ttraining's binary_logloss: 0.00252842\n",
      "[52]\ttraining's binary_logloss: 0.00250006\n",
      "[53]\ttraining's binary_logloss: 0.00247094\n",
      "[54]\ttraining's binary_logloss: 0.00244421\n",
      "[55]\ttraining's binary_logloss: 0.00241713\n",
      "[56]\ttraining's binary_logloss: 0.00238819\n",
      "[57]\ttraining's binary_logloss: 0.00236169\n",
      "[58]\ttraining's binary_logloss: 0.00233496\n",
      "[59]\ttraining's binary_logloss: 0.00231014\n",
      "[60]\ttraining's binary_logloss: 0.0022858\n",
      "[61]\ttraining's binary_logloss: 0.00225975\n",
      "[62]\ttraining's binary_logloss: 0.00223496\n",
      "[63]\ttraining's binary_logloss: 0.00221087\n",
      "[64]\ttraining's binary_logloss: 0.00218683\n",
      "[65]\ttraining's binary_logloss: 0.00216356\n",
      "[66]\ttraining's binary_logloss: 0.00214061\n",
      "[67]\ttraining's binary_logloss: 0.00211843\n",
      "[68]\ttraining's binary_logloss: 0.00209652\n",
      "[69]\ttraining's binary_logloss: 0.00207482\n",
      "[70]\ttraining's binary_logloss: 0.00205326\n",
      "[71]\ttraining's binary_logloss: 0.00202983\n",
      "[72]\ttraining's binary_logloss: 0.00200692\n",
      "[73]\ttraining's binary_logloss: 0.00198444\n",
      "[74]\ttraining's binary_logloss: 0.00196286\n",
      "[75]\ttraining's binary_logloss: 0.00194123\n",
      "[76]\ttraining's binary_logloss: 0.00192157\n",
      "[77]\ttraining's binary_logloss: 0.00190213\n",
      "[78]\ttraining's binary_logloss: 0.00188135\n",
      "[79]\ttraining's binary_logloss: 0.00186129\n",
      "[80]\ttraining's binary_logloss: 0.00184165\n",
      "[81]\ttraining's binary_logloss: 0.00182295\n",
      "[82]\ttraining's binary_logloss: 0.00180473\n",
      "[83]\ttraining's binary_logloss: 0.00178667\n",
      "[84]\ttraining's binary_logloss: 0.00176908\n",
      "[85]\ttraining's binary_logloss: 0.00175172\n",
      "[86]\ttraining's binary_logloss: 0.00173391\n",
      "[87]\ttraining's binary_logloss: 0.00171664\n",
      "[88]\ttraining's binary_logloss: 0.00169978\n",
      "[89]\ttraining's binary_logloss: 0.00168303\n",
      "[90]\ttraining's binary_logloss: 0.00166664\n",
      "[91]\ttraining's binary_logloss: 0.00165031\n",
      "[92]\ttraining's binary_logloss: 0.00163416\n",
      "[93]\ttraining's binary_logloss: 0.00161803\n",
      "[94]\ttraining's binary_logloss: 0.00160226\n",
      "[95]\ttraining's binary_logloss: 0.00158654\n",
      "[96]\ttraining's binary_logloss: 0.00156987\n",
      "[97]\ttraining's binary_logloss: 0.00155355\n",
      "[98]\ttraining's binary_logloss: 0.00153747\n",
      "[99]\ttraining's binary_logloss: 0.00152166\n",
      "[100]\ttraining's binary_logloss: 0.00150615\n",
      "Save model...\n",
      "Start predicting...\n",
      "Writing transformed training data\n",
      "Start training...\n",
      "[1]\ttraining's binary_logloss: 0.00608694\n",
      "[2]\ttraining's binary_logloss: 0.00578185\n",
      "[3]\ttraining's binary_logloss: 0.00555798\n",
      "[4]\ttraining's binary_logloss: 0.00540876\n",
      "[5]\ttraining's binary_logloss: 0.00525913\n",
      "[6]\ttraining's binary_logloss: 0.00512439\n",
      "[7]\ttraining's binary_logloss: 0.00500932\n",
      "[8]\ttraining's binary_logloss: 0.00489742\n",
      "[9]\ttraining's binary_logloss: 0.00479285\n",
      "[10]\ttraining's binary_logloss: 0.00469912\n",
      "[11]\ttraining's binary_logloss: 0.00459227\n",
      "[12]\ttraining's binary_logloss: 0.00449933\n",
      "[13]\ttraining's binary_logloss: 0.00441153\n",
      "[14]\ttraining's binary_logloss: 0.00432356\n",
      "[15]\ttraining's binary_logloss: 0.00424865\n",
      "[16]\ttraining's binary_logloss: 0.00415845\n",
      "[17]\ttraining's binary_logloss: 0.00407362\n",
      "[18]\ttraining's binary_logloss: 0.00400211\n",
      "[19]\ttraining's binary_logloss: 0.00393314\n",
      "[20]\ttraining's binary_logloss: 0.00386861\n",
      "[21]\ttraining's binary_logloss: 0.00380176\n",
      "[22]\ttraining's binary_logloss: 0.0037397\n",
      "[23]\ttraining's binary_logloss: 0.00368197\n",
      "[24]\ttraining's binary_logloss: 0.00362891\n",
      "[25]\ttraining's binary_logloss: 0.0035756\n",
      "[26]\ttraining's binary_logloss: 0.00351868\n",
      "[27]\ttraining's binary_logloss: 0.00346563\n",
      "[28]\ttraining's binary_logloss: 0.00341614\n",
      "[29]\ttraining's binary_logloss: 0.00336747\n",
      "[30]\ttraining's binary_logloss: 0.00332008\n",
      "[31]\ttraining's binary_logloss: 0.00327494\n",
      "[32]\ttraining's binary_logloss: 0.00322669\n",
      "[33]\ttraining's binary_logloss: 0.00318401\n",
      "[34]\ttraining's binary_logloss: 0.00314252\n",
      "[35]\ttraining's binary_logloss: 0.00309811\n",
      "[36]\ttraining's binary_logloss: 0.0030546\n",
      "[37]\ttraining's binary_logloss: 0.00301313\n",
      "[38]\ttraining's binary_logloss: 0.00297128\n",
      "[39]\ttraining's binary_logloss: 0.00293205\n",
      "[40]\ttraining's binary_logloss: 0.00289466\n",
      "[41]\ttraining's binary_logloss: 0.00285809\n",
      "[42]\ttraining's binary_logloss: 0.00282402\n",
      "[43]\ttraining's binary_logloss: 0.00278886\n",
      "[44]\ttraining's binary_logloss: 0.0027554\n",
      "[45]\ttraining's binary_logloss: 0.00272396\n",
      "[46]\ttraining's binary_logloss: 0.00268889\n",
      "[47]\ttraining's binary_logloss: 0.00265594\n",
      "[48]\ttraining's binary_logloss: 0.00262267\n",
      "[49]\ttraining's binary_logloss: 0.00258951\n",
      "[50]\ttraining's binary_logloss: 0.0025591\n",
      "[51]\ttraining's binary_logloss: 0.00252842\n",
      "[52]\ttraining's binary_logloss: 0.00250006\n",
      "[53]\ttraining's binary_logloss: 0.00247094\n",
      "[54]\ttraining's binary_logloss: 0.00244421\n",
      "[55]\ttraining's binary_logloss: 0.00241713\n",
      "[56]\ttraining's binary_logloss: 0.00238819\n",
      "[57]\ttraining's binary_logloss: 0.00236169\n",
      "[58]\ttraining's binary_logloss: 0.00233496\n",
      "[59]\ttraining's binary_logloss: 0.00231014\n",
      "[60]\ttraining's binary_logloss: 0.0022858\n",
      "[61]\ttraining's binary_logloss: 0.00225975\n",
      "[62]\ttraining's binary_logloss: 0.00223496\n",
      "[63]\ttraining's binary_logloss: 0.00221087\n",
      "[64]\ttraining's binary_logloss: 0.00218683\n",
      "[65]\ttraining's binary_logloss: 0.00216356\n",
      "[66]\ttraining's binary_logloss: 0.00214061\n",
      "[67]\ttraining's binary_logloss: 0.00211843\n",
      "[68]\ttraining's binary_logloss: 0.00209652\n",
      "[69]\ttraining's binary_logloss: 0.00207482\n",
      "[70]\ttraining's binary_logloss: 0.00205326\n",
      "[71]\ttraining's binary_logloss: 0.00202983\n",
      "[72]\ttraining's binary_logloss: 0.00200692\n",
      "[73]\ttraining's binary_logloss: 0.00198444\n",
      "[74]\ttraining's binary_logloss: 0.00196286\n",
      "[75]\ttraining's binary_logloss: 0.00194123\n",
      "[76]\ttraining's binary_logloss: 0.00192157\n",
      "[77]\ttraining's binary_logloss: 0.00190213\n",
      "[78]\ttraining's binary_logloss: 0.00188135\n",
      "[79]\ttraining's binary_logloss: 0.00186129\n",
      "[80]\ttraining's binary_logloss: 0.00184165\n",
      "[81]\ttraining's binary_logloss: 0.00182295\n",
      "[82]\ttraining's binary_logloss: 0.00180473\n",
      "[83]\ttraining's binary_logloss: 0.00178667\n",
      "[84]\ttraining's binary_logloss: 0.00176908\n",
      "[85]\ttraining's binary_logloss: 0.00175172\n",
      "[86]\ttraining's binary_logloss: 0.00173391\n",
      "[87]\ttraining's binary_logloss: 0.00171664\n",
      "[88]\ttraining's binary_logloss: 0.00169978\n",
      "[89]\ttraining's binary_logloss: 0.00168303\n",
      "[90]\ttraining's binary_logloss: 0.00166664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91]\ttraining's binary_logloss: 0.00165031\n",
      "[92]\ttraining's binary_logloss: 0.00163416\n",
      "[93]\ttraining's binary_logloss: 0.00161803\n",
      "[94]\ttraining's binary_logloss: 0.00160226\n",
      "[95]\ttraining's binary_logloss: 0.00158654\n",
      "[96]\ttraining's binary_logloss: 0.00156987\n",
      "[97]\ttraining's binary_logloss: 0.00155355\n",
      "[98]\ttraining's binary_logloss: 0.00153747\n",
      "[99]\ttraining's binary_logloss: 0.00152166\n",
      "[100]\ttraining's binary_logloss: 0.00150615\n",
      "Save model...\n",
      "Start predicting...\n",
      "Writing transformed training data\n",
      "Start training...\n",
      "[1]\ttraining's binary_logloss: 0.00608694\n",
      "[2]\ttraining's binary_logloss: 0.00578185\n",
      "[3]\ttraining's binary_logloss: 0.00555798\n",
      "[4]\ttraining's binary_logloss: 0.00540876\n",
      "[5]\ttraining's binary_logloss: 0.00525913\n",
      "[6]\ttraining's binary_logloss: 0.00512439\n",
      "[7]\ttraining's binary_logloss: 0.00500932\n",
      "[8]\ttraining's binary_logloss: 0.00489742\n",
      "[9]\ttraining's binary_logloss: 0.00479285\n",
      "[10]\ttraining's binary_logloss: 0.00469912\n",
      "[11]\ttraining's binary_logloss: 0.00459227\n",
      "[12]\ttraining's binary_logloss: 0.00449933\n",
      "[13]\ttraining's binary_logloss: 0.00441153\n",
      "[14]\ttraining's binary_logloss: 0.00432356\n",
      "[15]\ttraining's binary_logloss: 0.00424865\n",
      "[16]\ttraining's binary_logloss: 0.00415845\n",
      "[17]\ttraining's binary_logloss: 0.00407362\n",
      "[18]\ttraining's binary_logloss: 0.00400211\n",
      "[19]\ttraining's binary_logloss: 0.00393314\n",
      "[20]\ttraining's binary_logloss: 0.00386861\n",
      "[21]\ttraining's binary_logloss: 0.00380176\n",
      "[22]\ttraining's binary_logloss: 0.0037397\n",
      "[23]\ttraining's binary_logloss: 0.00368197\n",
      "[24]\ttraining's binary_logloss: 0.00362891\n",
      "[25]\ttraining's binary_logloss: 0.0035756\n",
      "[26]\ttraining's binary_logloss: 0.00351868\n",
      "[27]\ttraining's binary_logloss: 0.00346563\n",
      "[28]\ttraining's binary_logloss: 0.00341614\n",
      "[29]\ttraining's binary_logloss: 0.00336747\n",
      "[30]\ttraining's binary_logloss: 0.00332008\n",
      "[31]\ttraining's binary_logloss: 0.00327494\n",
      "[32]\ttraining's binary_logloss: 0.00322669\n",
      "[33]\ttraining's binary_logloss: 0.00318401\n",
      "[34]\ttraining's binary_logloss: 0.00314252\n",
      "[35]\ttraining's binary_logloss: 0.00309811\n",
      "[36]\ttraining's binary_logloss: 0.0030546\n",
      "[37]\ttraining's binary_logloss: 0.00301313\n",
      "[38]\ttraining's binary_logloss: 0.00297128\n",
      "[39]\ttraining's binary_logloss: 0.00293205\n",
      "[40]\ttraining's binary_logloss: 0.00289466\n",
      "[41]\ttraining's binary_logloss: 0.00285809\n",
      "[42]\ttraining's binary_logloss: 0.00282402\n",
      "[43]\ttraining's binary_logloss: 0.00278886\n",
      "[44]\ttraining's binary_logloss: 0.0027554\n",
      "[45]\ttraining's binary_logloss: 0.00272396\n",
      "[46]\ttraining's binary_logloss: 0.00268889\n",
      "[47]\ttraining's binary_logloss: 0.00265594\n",
      "[48]\ttraining's binary_logloss: 0.00262267\n",
      "[49]\ttraining's binary_logloss: 0.00258951\n",
      "[50]\ttraining's binary_logloss: 0.0025591\n",
      "[51]\ttraining's binary_logloss: 0.00252842\n",
      "[52]\ttraining's binary_logloss: 0.00250006\n",
      "[53]\ttraining's binary_logloss: 0.00247094\n",
      "[54]\ttraining's binary_logloss: 0.00244421\n",
      "[55]\ttraining's binary_logloss: 0.00241713\n",
      "[56]\ttraining's binary_logloss: 0.00238819\n",
      "[57]\ttraining's binary_logloss: 0.00236169\n",
      "[58]\ttraining's binary_logloss: 0.00233496\n",
      "[59]\ttraining's binary_logloss: 0.00231014\n",
      "[60]\ttraining's binary_logloss: 0.0022858\n",
      "[61]\ttraining's binary_logloss: 0.00225975\n",
      "[62]\ttraining's binary_logloss: 0.00223496\n",
      "[63]\ttraining's binary_logloss: 0.00221087\n",
      "[64]\ttraining's binary_logloss: 0.00218683\n",
      "[65]\ttraining's binary_logloss: 0.00216356\n",
      "[66]\ttraining's binary_logloss: 0.00214061\n",
      "[67]\ttraining's binary_logloss: 0.00211843\n",
      "[68]\ttraining's binary_logloss: 0.00209652\n",
      "[69]\ttraining's binary_logloss: 0.00207482\n",
      "[70]\ttraining's binary_logloss: 0.00205326\n",
      "[71]\ttraining's binary_logloss: 0.00202983\n",
      "[72]\ttraining's binary_logloss: 0.00200692\n",
      "[73]\ttraining's binary_logloss: 0.00198444\n",
      "[74]\ttraining's binary_logloss: 0.00196286\n",
      "[75]\ttraining's binary_logloss: 0.00194123\n",
      "[76]\ttraining's binary_logloss: 0.00192157\n",
      "[77]\ttraining's binary_logloss: 0.00190213\n",
      "[78]\ttraining's binary_logloss: 0.00188135\n",
      "[79]\ttraining's binary_logloss: 0.00186129\n",
      "[80]\ttraining's binary_logloss: 0.00184165\n",
      "[81]\ttraining's binary_logloss: 0.00182295\n",
      "[82]\ttraining's binary_logloss: 0.00180473\n",
      "[83]\ttraining's binary_logloss: 0.00178667\n",
      "[84]\ttraining's binary_logloss: 0.00176908\n",
      "[85]\ttraining's binary_logloss: 0.00175172\n",
      "[86]\ttraining's binary_logloss: 0.00173391\n",
      "[87]\ttraining's binary_logloss: 0.00171664\n",
      "[88]\ttraining's binary_logloss: 0.00169978\n",
      "[89]\ttraining's binary_logloss: 0.00168303\n",
      "[90]\ttraining's binary_logloss: 0.00166664\n",
      "[91]\ttraining's binary_logloss: 0.00165031\n",
      "[92]\ttraining's binary_logloss: 0.00163416\n",
      "[93]\ttraining's binary_logloss: 0.00161803\n",
      "[94]\ttraining's binary_logloss: 0.00160226\n",
      "[95]\ttraining's binary_logloss: 0.00158654\n",
      "[96]\ttraining's binary_logloss: 0.00156987\n",
      "[97]\ttraining's binary_logloss: 0.00155355\n",
      "[98]\ttraining's binary_logloss: 0.00153747\n",
      "[99]\ttraining's binary_logloss: 0.00152166\n",
      "[100]\ttraining's binary_logloss: 0.00150615\n",
      "Save model...\n",
      "Start predicting...\n",
      "Writing transformed training data\n",
      "Start training...\n",
      "[1]\ttraining's binary_logloss: 0.00608694\n",
      "[2]\ttraining's binary_logloss: 0.00578185\n",
      "[3]\ttraining's binary_logloss: 0.00555798\n",
      "[4]\ttraining's binary_logloss: 0.00540876\n",
      "[5]\ttraining's binary_logloss: 0.00525913\n",
      "[6]\ttraining's binary_logloss: 0.00512439\n",
      "[7]\ttraining's binary_logloss: 0.00500932\n",
      "[8]\ttraining's binary_logloss: 0.00489742\n",
      "[9]\ttraining's binary_logloss: 0.00479285\n",
      "[10]\ttraining's binary_logloss: 0.00469912\n",
      "[11]\ttraining's binary_logloss: 0.00459227\n",
      "[12]\ttraining's binary_logloss: 0.00449933\n",
      "[13]\ttraining's binary_logloss: 0.00441153\n",
      "[14]\ttraining's binary_logloss: 0.00432356\n",
      "[15]\ttraining's binary_logloss: 0.00424865\n",
      "[16]\ttraining's binary_logloss: 0.00415845\n",
      "[17]\ttraining's binary_logloss: 0.00407362\n",
      "[18]\ttraining's binary_logloss: 0.00400211\n",
      "[19]\ttraining's binary_logloss: 0.00393314\n",
      "[20]\ttraining's binary_logloss: 0.00386861\n",
      "[21]\ttraining's binary_logloss: 0.00380176\n",
      "[22]\ttraining's binary_logloss: 0.0037397\n",
      "[23]\ttraining's binary_logloss: 0.00368197\n",
      "[24]\ttraining's binary_logloss: 0.00362891\n",
      "[25]\ttraining's binary_logloss: 0.0035756\n",
      "[26]\ttraining's binary_logloss: 0.00351868\n",
      "[27]\ttraining's binary_logloss: 0.00346563\n",
      "[28]\ttraining's binary_logloss: 0.00341614\n",
      "[29]\ttraining's binary_logloss: 0.00336747\n",
      "[30]\ttraining's binary_logloss: 0.00332008\n",
      "[31]\ttraining's binary_logloss: 0.00327494\n",
      "[32]\ttraining's binary_logloss: 0.00322669\n",
      "[33]\ttraining's binary_logloss: 0.00318401\n",
      "[34]\ttraining's binary_logloss: 0.00314252\n",
      "[35]\ttraining's binary_logloss: 0.00309811\n",
      "[36]\ttraining's binary_logloss: 0.0030546\n",
      "[37]\ttraining's binary_logloss: 0.00301313\n",
      "[38]\ttraining's binary_logloss: 0.00297128\n",
      "[39]\ttraining's binary_logloss: 0.00293205\n",
      "[40]\ttraining's binary_logloss: 0.00289466\n",
      "[41]\ttraining's binary_logloss: 0.00285809\n",
      "[42]\ttraining's binary_logloss: 0.00282402\n",
      "[43]\ttraining's binary_logloss: 0.00278886\n",
      "[44]\ttraining's binary_logloss: 0.0027554\n",
      "[45]\ttraining's binary_logloss: 0.00272396\n",
      "[46]\ttraining's binary_logloss: 0.00268889\n",
      "[47]\ttraining's binary_logloss: 0.00265594\n",
      "[48]\ttraining's binary_logloss: 0.00262267\n",
      "[49]\ttraining's binary_logloss: 0.00258951\n",
      "[50]\ttraining's binary_logloss: 0.0025591\n",
      "[51]\ttraining's binary_logloss: 0.00252842\n",
      "[52]\ttraining's binary_logloss: 0.00250006\n",
      "[53]\ttraining's binary_logloss: 0.00247094\n",
      "[54]\ttraining's binary_logloss: 0.00244421\n",
      "[55]\ttraining's binary_logloss: 0.00241713\n",
      "[56]\ttraining's binary_logloss: 0.00238819\n",
      "[57]\ttraining's binary_logloss: 0.00236169\n",
      "[58]\ttraining's binary_logloss: 0.00233496\n",
      "[59]\ttraining's binary_logloss: 0.00231014\n",
      "[60]\ttraining's binary_logloss: 0.0022858\n",
      "[61]\ttraining's binary_logloss: 0.00225975\n",
      "[62]\ttraining's binary_logloss: 0.00223496\n",
      "[63]\ttraining's binary_logloss: 0.00221087\n",
      "[64]\ttraining's binary_logloss: 0.00218683\n",
      "[65]\ttraining's binary_logloss: 0.00216356\n",
      "[66]\ttraining's binary_logloss: 0.00214061\n",
      "[67]\ttraining's binary_logloss: 0.00211843\n",
      "[68]\ttraining's binary_logloss: 0.00209652\n",
      "[69]\ttraining's binary_logloss: 0.00207482\n",
      "[70]\ttraining's binary_logloss: 0.00205326\n",
      "[71]\ttraining's binary_logloss: 0.00202983\n",
      "[72]\ttraining's binary_logloss: 0.00200692\n",
      "[73]\ttraining's binary_logloss: 0.00198444\n",
      "[74]\ttraining's binary_logloss: 0.00196286\n",
      "[75]\ttraining's binary_logloss: 0.00194123\n",
      "[76]\ttraining's binary_logloss: 0.00192157\n",
      "[77]\ttraining's binary_logloss: 0.00190213\n",
      "[78]\ttraining's binary_logloss: 0.00188135\n",
      "[79]\ttraining's binary_logloss: 0.00186129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80]\ttraining's binary_logloss: 0.00184165\n",
      "[81]\ttraining's binary_logloss: 0.00182295\n",
      "[82]\ttraining's binary_logloss: 0.00180473\n",
      "[83]\ttraining's binary_logloss: 0.00178667\n",
      "[84]\ttraining's binary_logloss: 0.00176908\n",
      "[85]\ttraining's binary_logloss: 0.00175172\n",
      "[86]\ttraining's binary_logloss: 0.00173391\n",
      "[87]\ttraining's binary_logloss: 0.00171664\n",
      "[88]\ttraining's binary_logloss: 0.00169978\n",
      "[89]\ttraining's binary_logloss: 0.00168303\n",
      "[90]\ttraining's binary_logloss: 0.00166664\n",
      "[91]\ttraining's binary_logloss: 0.00165031\n",
      "[92]\ttraining's binary_logloss: 0.00163416\n",
      "[93]\ttraining's binary_logloss: 0.00161803\n",
      "[94]\ttraining's binary_logloss: 0.00160226\n",
      "[95]\ttraining's binary_logloss: 0.00158654\n",
      "[96]\ttraining's binary_logloss: 0.00156987\n",
      "[97]\ttraining's binary_logloss: 0.00155355\n",
      "[98]\ttraining's binary_logloss: 0.00153747\n",
      "[99]\ttraining's binary_logloss: 0.00152166\n",
      "[100]\ttraining's binary_logloss: 0.00150615\n",
      "Save model...\n",
      "Start predicting...\n",
      "Writing transformed training data\n",
      "Start training...\n",
      "[1]\ttraining's binary_logloss: 0.00608694\n",
      "[2]\ttraining's binary_logloss: 0.00578185\n",
      "[3]\ttraining's binary_logloss: 0.00555798\n",
      "[4]\ttraining's binary_logloss: 0.00540876\n",
      "[5]\ttraining's binary_logloss: 0.00525913\n",
      "[6]\ttraining's binary_logloss: 0.00512439\n",
      "[7]\ttraining's binary_logloss: 0.00500932\n",
      "[8]\ttraining's binary_logloss: 0.00489742\n",
      "[9]\ttraining's binary_logloss: 0.00479285\n",
      "[10]\ttraining's binary_logloss: 0.00469912\n",
      "[11]\ttraining's binary_logloss: 0.00459227\n",
      "[12]\ttraining's binary_logloss: 0.00449933\n",
      "[13]\ttraining's binary_logloss: 0.00441153\n",
      "[14]\ttraining's binary_logloss: 0.00432356\n",
      "[15]\ttraining's binary_logloss: 0.00424865\n",
      "[16]\ttraining's binary_logloss: 0.00415845\n",
      "[17]\ttraining's binary_logloss: 0.00407362\n",
      "[18]\ttraining's binary_logloss: 0.00400211\n",
      "[19]\ttraining's binary_logloss: 0.00393314\n",
      "[20]\ttraining's binary_logloss: 0.00386861\n",
      "[21]\ttraining's binary_logloss: 0.00380176\n",
      "[22]\ttraining's binary_logloss: 0.0037397\n",
      "[23]\ttraining's binary_logloss: 0.00368197\n",
      "[24]\ttraining's binary_logloss: 0.00362891\n",
      "[25]\ttraining's binary_logloss: 0.0035756\n",
      "[26]\ttraining's binary_logloss: 0.00351868\n",
      "[27]\ttraining's binary_logloss: 0.00346563\n",
      "[28]\ttraining's binary_logloss: 0.00341614\n",
      "[29]\ttraining's binary_logloss: 0.00336747\n",
      "[30]\ttraining's binary_logloss: 0.00332008\n",
      "[31]\ttraining's binary_logloss: 0.00327494\n",
      "[32]\ttraining's binary_logloss: 0.00322669\n",
      "[33]\ttraining's binary_logloss: 0.00318401\n",
      "[34]\ttraining's binary_logloss: 0.00314252\n",
      "[35]\ttraining's binary_logloss: 0.00309811\n",
      "[36]\ttraining's binary_logloss: 0.0030546\n",
      "[37]\ttraining's binary_logloss: 0.00301313\n",
      "[38]\ttraining's binary_logloss: 0.00297128\n",
      "[39]\ttraining's binary_logloss: 0.00293205\n",
      "[40]\ttraining's binary_logloss: 0.00289466\n",
      "[41]\ttraining's binary_logloss: 0.00285809\n",
      "[42]\ttraining's binary_logloss: 0.00282402\n",
      "[43]\ttraining's binary_logloss: 0.00278886\n",
      "[44]\ttraining's binary_logloss: 0.0027554\n",
      "[45]\ttraining's binary_logloss: 0.00272396\n",
      "[46]\ttraining's binary_logloss: 0.00268889\n",
      "[47]\ttraining's binary_logloss: 0.00265594\n",
      "[48]\ttraining's binary_logloss: 0.00262267\n",
      "[49]\ttraining's binary_logloss: 0.00258951\n",
      "[50]\ttraining's binary_logloss: 0.0025591\n",
      "[51]\ttraining's binary_logloss: 0.00252842\n",
      "[52]\ttraining's binary_logloss: 0.00250006\n",
      "[53]\ttraining's binary_logloss: 0.00247094\n",
      "[54]\ttraining's binary_logloss: 0.00244421\n",
      "[55]\ttraining's binary_logloss: 0.00241713\n",
      "[56]\ttraining's binary_logloss: 0.00238819\n",
      "[57]\ttraining's binary_logloss: 0.00236169\n",
      "[58]\ttraining's binary_logloss: 0.00233496\n",
      "[59]\ttraining's binary_logloss: 0.00231014\n",
      "[60]\ttraining's binary_logloss: 0.0022858\n",
      "[61]\ttraining's binary_logloss: 0.00225975\n",
      "[62]\ttraining's binary_logloss: 0.00223496\n",
      "[63]\ttraining's binary_logloss: 0.00221087\n",
      "[64]\ttraining's binary_logloss: 0.00218683\n",
      "[65]\ttraining's binary_logloss: 0.00216356\n",
      "[66]\ttraining's binary_logloss: 0.00214061\n",
      "[67]\ttraining's binary_logloss: 0.00211843\n",
      "[68]\ttraining's binary_logloss: 0.00209652\n",
      "[69]\ttraining's binary_logloss: 0.00207482\n",
      "[70]\ttraining's binary_logloss: 0.00205326\n",
      "[71]\ttraining's binary_logloss: 0.00202983\n",
      "[72]\ttraining's binary_logloss: 0.00200692\n",
      "[73]\ttraining's binary_logloss: 0.00198444\n",
      "[74]\ttraining's binary_logloss: 0.00196286\n",
      "[75]\ttraining's binary_logloss: 0.00194123\n",
      "[76]\ttraining's binary_logloss: 0.00192157\n",
      "[77]\ttraining's binary_logloss: 0.00190213\n",
      "[78]\ttraining's binary_logloss: 0.00188135\n",
      "[79]\ttraining's binary_logloss: 0.00186129\n",
      "[80]\ttraining's binary_logloss: 0.00184165\n",
      "[81]\ttraining's binary_logloss: 0.00182295\n",
      "[82]\ttraining's binary_logloss: 0.00180473\n",
      "[83]\ttraining's binary_logloss: 0.00178667\n",
      "[84]\ttraining's binary_logloss: 0.00176908\n",
      "[85]\ttraining's binary_logloss: 0.00175172\n",
      "[86]\ttraining's binary_logloss: 0.00173391\n",
      "[87]\ttraining's binary_logloss: 0.00171664\n",
      "[88]\ttraining's binary_logloss: 0.00169978\n",
      "[89]\ttraining's binary_logloss: 0.00168303\n",
      "[90]\ttraining's binary_logloss: 0.00166664\n",
      "[91]\ttraining's binary_logloss: 0.00165031\n",
      "[92]\ttraining's binary_logloss: 0.00163416\n",
      "[93]\ttraining's binary_logloss: 0.00161803\n",
      "[94]\ttraining's binary_logloss: 0.00160226\n",
      "[95]\ttraining's binary_logloss: 0.00158654\n",
      "[96]\ttraining's binary_logloss: 0.00156987\n",
      "[97]\ttraining's binary_logloss: 0.00155355\n",
      "[98]\ttraining's binary_logloss: 0.00153747\n",
      "[99]\ttraining's binary_logloss: 0.00152166\n",
      "[100]\ttraining's binary_logloss: 0.00150615\n",
      "Save model...\n",
      "Start predicting...\n",
      "Writing transformed training data\n"
     ]
    }
   ],
   "source": [
    "LightGBM_GBDT_plus_LR(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
